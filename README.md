# 传统统计学习方法实现文本分类
传统统计学习方法实现文本分类
基于朴素贝叶斯的文本分类方法

一、朴素贝叶斯原理的介绍

二、朴素贝叶斯分类器的代码实现

分类器有时会产生错误结果，这时可以要求分类器给出一个最优的类别猜测结果，同时会给出这个猜测的概率估计值。朴素贝叶斯分类是使用概率论进行分类的方法。所谓“朴素”，是因为整个形式化过程只做最原始、最简单的假设。



一、朴素贝叶斯原理的介绍

优点：在数据较少的情况下仍然有效，可以处理多类别问题。

缺点：对于输入数据的准备方式较为敏感。

适用数据类型：标称型数据。

1.贝叶斯决策理论

在学习朴素贝叶斯分类之前，我们先回顾一下贝叶斯决策理论和条件概率。贝叶斯决策理论的核心思想是选择最高概率对应的类别，也就是选择具有最高概率的决策。贝叶斯准则是计算条件概率的有效方法，可以告诉我们如何交换条件概率中的条件与结果，即如果已知P(x|C),要求P(C|x)，那么就可以使用下面的计算方法：

[公式]

2.使用条件概率分类

假设我们有一个二维数据集，由两类数据组成，现在我们结合贝叶斯决策理论使用条件概率来给这些数据点分类。即给定数据点(x,y)，判断是属于类别1还是类别2的方法是，分别计算该数据点来自类别1和来自类别2的概率。所以真正需要比较的是P(C1|x,y)和P(C2|x,y)。若已知从给定类别中取出该数据的概率，即P(x,y|Ci) ，应用贝叶斯准则可以得到：[公式]

当 [公式] ，那么属于类别1，

当 [公式] ，则属于类别2。

以上是贝叶斯准则的简要理论，我们在对文档进行分的类常用算法朴素贝叶斯分类器中“朴素”一词基于两个假设：

a.特征之间相互独立，在上述二维数据中的体现就是x与y相互独立。

b.每个特征同等重要。即x与y有同样的重要性。

在应用于文档分类时，尽管很多情况下特征难以完全符合上述假设，但朴素贝叶斯的实际效果却很好。

下面我们将其付诸实践，使用Python构建基于这些理论的分类器。

二、朴素贝叶斯分类器的实现

在文档分类中，整个文档（如一封电子邮件）是实例，文档中的某些元素构成特征。我们把每个词的出现或不出现作为一个特征。整个文本分类的过程分为三步：

a.拆分文本，获取特征。在英文文档分类中，特征来自文本的词条（token）,一个词条可以是单词，也可以是URL、IP地址等任意其他字符串。

b.将文本数字化，构建词向量。每一个文本片段表示为一个词条向量，其中值为1表示词条出现在文档中，0表示词条未出现。

c.计算条件概率并分类。应用朴素贝叶斯分类原理，通过词向量计算条件概率。这里w是一个向量，由多个数值组成，它代表着由多个单词组成的一段文本或者一组单词。

[公式]

计算上面的概率值，然后比较大小。

下面是在线社区留言板的案例，我们要留言分为侮辱类和非侮辱类。如果留言中使用了负面或者侮辱性词语，那么该留言就识别为侮辱类留言，记为1，否则0。
1.准备数据，从文本中构建词向量（one_hot向量）
先将所有文档中出现过的词构建词汇表，然后把每篇文档中的词转换成词汇表上的向量。由于这里我们用的是将一个词出现与否作为特征的词集模型（对应的还有词袋模型），不计数每个词出现的次数，所以词向量中只需将出现在词汇表的词记为1即可。
2.从词向量计算概率

上面我们已经探讨了，要得到类别概率值，首先要计算P(w|Ci)和P(Ci)。P(Ci)容易获得，用侮辱类文档数除以总文档数即可。接下来计算P(w|Ci)。朴素贝叶斯中假设特征相互独立，这里体现为每个词条相互独立。将P(w|Ci)展开成P(w0,w1,w2,...,wn|Ci)，根据假设P(w0,w1,w2,...,wn|Ci)=P(w0|Ci)P(w1|Ci)P(w2|Ci)...P(wn|Ci)成立，我们可以以此计算上述概率。

伪代码如下：

计算每个类别文档数目

对每篇训练文档：

对每个类别：

如果词条出现在文档中，则增加该词的计数值

增加该类文档所有词的计数值

对每个类别：

将该词的计数值除以该类文档总词数得到条件概率

返回每个类别的条件概率
3.测试运行结果
4.简单应用词袋模型
